# Session Assessment

**Session ID:** [unique identifier]  
**Date:** [date]  
**Setup:** [ ] Vanilla (Cursor+Opus+MCP only)  /  [ ] Framework  
**Script/Task:** [what task was given]  
**Duration:** [total time]  

---

## 1. Session Summary

> **Prompt:** Summarize this session: What was the main task/goal? What outputs did we create? How many iterations did the main deliverable go through? What was your understanding of my context?

**Agent Response:**
```
[paste response here]
```

---

## 2. Context Awareness Check

> **Prompt:** Without looking at any files, answer: What team/domain am I working in? What are my current priorities? Who are the key stakeholders? What constraints shaped our work? Rate your confidence.

**Agent Response:**
```
[paste response here]
```

---

## 3. Output Self-Assessment

> **Prompt:** Look at the main output(s) we created. Is it ready to share externally? What's missing? Did you reference actual code, docs, or data? Rate quality 1-5.

**Agent Response:**
```
[paste response here]
```

---

## 4. Process Reflection

> **Prompt:** Reflect on how we worked together: How many times did I need to re-explain or correct you? Did I provide structure or did you propose it? What mode/approach did you take? What would have made this more efficient?

**Agent Response:**
```
[paste response here]
```

---

## 5. Grounding Check

> **Prompt:** List every specific file, system, codebase, data point, metric, person, team, or doc you referenced. For each, note: Was this from session context or general knowledge?

**Agent Response:**
```
[paste response here]
```

---

## 6. Next Steps Awareness

> **Prompt:** Based on our session: What should I do next? What follow-up work would naturally come from this? What questions remain? Who should I share this with?

**Agent Response:**
```
[paste response here]
```

---

## Evaluator Notes

### Quantitative Metrics

| Metric | Value |
|--------|-------|
| Total prompts sent | |
| Time to first usable output | min |
| Total session time | min |
| Files/artifacts created | |
| Iterations on main deliverable | |
| Times context was re-explained | |

### Qualitative Ratings (1-5)

| Dimension | Rating | Notes |
|-----------|--------|-------|
| Output quality (shareable as-is) | /5 | |
| Context awareness | /5 | |
| Structure & organization | /5 | |
| Proactivity (anticipated needs) | /5 | |
| Grounding (real refs vs generic) | /5 | |
| Mode appropriateness | /5 | |

### Open Observations

**What worked well:**
- 

**What was frustrating:**
- 

**Biggest difference noticed (if comparing):**
- 

---

## Artifacts Created

| File | Description | Quality (1-5) |
|------|-------------|---------------|
| | | |
| | | |
| | | |

---

*Assessment completed by: [name]*  
*Date: [date]*
